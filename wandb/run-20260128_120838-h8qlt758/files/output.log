
Epoch 1/20 - Training:
Number of events per file [6080806]
Number of events per file [6080806]
Number of events per file [6080806]
/global/homes/g/gregork/.conda/envs/omni/lib/python3.12/site-packages/torch/autograd/graph.py:865: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 4, 128], strides() = [19712, 128, 1]
bucket_view.sizes() = [1, 4, 128], strides() = [512, 128, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:330.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  [10/6080806] (0.0%) - Loss: 8.8167, LR: 5.00e-05
  [20/6080806] (0.0%) - Loss: 3.4517, LR: 5.00e-05
  [30/6080806] (0.0%) - Loss: 11.5374, LR: 5.00e-05
  [40/6080806] (0.0%) - Loss: 5.8502, LR: 5.00e-05
  [50/6080806] (0.0%) - Loss: 4.2602, LR: 5.00e-05
  [60/6080806] (0.0%) - Loss: 3.5555, LR: 5.00e-05
  [70/6080806] (0.0%) - Loss: 5.0179, LR: 5.00e-05
  [80/6080806] (0.0%) - Loss: 4.7003, LR: 5.00e-05
  [90/6080806] (0.0%) - Loss: 1.7376, LR: 5.00e-05
  [100/6080806] (0.0%) - Loss: 2.2399, LR: 5.00e-05
  [110/6080806] (0.0%) - Loss: 6.7217, LR: 5.00e-05
  [120/6080806] (0.0%) - Loss: 4.6568, LR: 5.00e-05
  [130/6080806] (0.0%) - Loss: 2.9397, LR: 5.00e-05
  [140/6080806] (0.0%) - Loss: 4.7604, LR: 5.00e-05
  [150/6080806] (0.0%) - Loss: 0.9790, LR: 5.00e-05
  [160/6080806] (0.0%) - Loss: 8.4947, LR: 5.00e-05
  [170/6080806] (0.0%) - Loss: 4.8379, LR: 5.00e-05
  [180/6080806] (0.0%) - Loss: 7.1776, LR: 5.00e-05
  [190/6080806] (0.0%) - Loss: 5.8675, LR: 5.00e-05
  [200/6080806] (0.0%) - Loss: 3.2999, LR: 5.00e-05
  [210/6080806] (0.0%) - Loss: 1.7771, LR: 5.00e-05
  [220/6080806] (0.0%) - Loss: 2.4882, LR: 5.00e-05
  [230/6080806] (0.0%) - Loss: 6.8265, LR: 5.00e-05
  [240/6080806] (0.0%) - Loss: 1.8169, LR: 5.00e-05
  [250/6080806] (0.0%) - Loss: 5.4417, LR: 5.00e-05
  [260/6080806] (0.0%) - Loss: 4.0136, LR: 5.00e-05
  [270/6080806] (0.0%) - Loss: 1.2647, LR: 5.00e-05
  [280/6080806] (0.0%) - Loss: 3.9974, LR: 5.00e-05
  [290/6080806] (0.0%) - Loss: 5.1423, LR: 5.00e-05
  [300/6080806] (0.0%) - Loss: 7.9890, LR: 5.00e-05
  [310/6080806] (0.0%) - Loss: 4.8426, LR: 5.00e-05
  [320/6080806] (0.0%) - Loss: 1.4605, LR: 5.00e-05
  [330/6080806] (0.0%) - Loss: 4.8281, LR: 5.00e-05
  [340/6080806] (0.0%) - Loss: 2.8679, LR: 5.00e-05
  [350/6080806] (0.0%) - Loss: 5.1420, LR: 5.00e-05
  [360/6080806] (0.0%) - Loss: 1.4434, LR: 5.00e-05
  [370/6080806] (0.0%) - Loss: 1.4027, LR: 5.00e-05
  [380/6080806] (0.0%) - Loss: 5.5688, LR: 5.00e-05
  [390/6080806] (0.0%) - Loss: 2.3033, LR: 5.00e-05
  [400/6080806] (0.0%) - Loss: 4.0080, LR: 5.00e-05
  [410/6080806] (0.0%) - Loss: 3.1128, LR: 5.00e-05
  [420/6080806] (0.0%) - Loss: 4.0323, LR: 5.00e-05
  [430/6080806] (0.0%) - Loss: 2.3215, LR: 5.00e-05
  [440/6080806] (0.0%) - Loss: 2.4925, LR: 5.00e-05
  [450/6080806] (0.0%) - Loss: 5.4054, LR: 5.00e-05
  [460/6080806] (0.0%) - Loss: 6.6108, LR: 5.00e-05
  [470/6080806] (0.0%) - Loss: 2.2167, LR: 5.00e-05
  [480/6080806] (0.0%) - Loss: 1.4064, LR: 5.00e-05
  [490/6080806] (0.0%) - Loss: 1.6910, LR: 5.00e-05
  [500/6080806] (0.0%) - Loss: 5.0135, LR: 5.00e-05
  [510/6080806] (0.0%) - Loss: 2.8593, LR: 5.00e-05
  [520/6080806] (0.0%) - Loss: 6.5447, LR: 5.00e-05
  [530/6080806] (0.0%) - Loss: 8.7953, LR: 5.00e-05
  [540/6080806] (0.0%) - Loss: 5.6469, LR: 5.00e-05
  [550/6080806] (0.0%) - Loss: 4.0996, LR: 5.00e-05
  [560/6080806] (0.0%) - Loss: 1.9683, LR: 5.00e-05
  [570/6080806] (0.0%) - Loss: 3.8375, LR: 5.00e-05
  [580/6080806] (0.0%) - Loss: 4.9840, LR: 5.00e-05
  [590/6080806] (0.0%) - Loss: 3.8191, LR: 5.00e-05
  [600/6080806] (0.0%) - Loss: 3.7591, LR: 5.00e-05
  [610/6080806] (0.0%) - Loss: 1.8888, LR: 5.00e-05
  [620/6080806] (0.0%) - Loss: 2.4961, LR: 5.00e-05
  [630/6080806] (0.0%) - Loss: 4.4476, LR: 5.00e-05
  [640/6080806] (0.0%) - Loss: 6.4829, LR: 5.00e-05
  [650/6080806] (0.0%) - Loss: 1.7248, LR: 5.00e-05
  [660/6080806] (0.0%) - Loss: 6.1738, LR: 5.00e-05
  [670/6080806] (0.0%) - Loss: 3.2410, LR: 5.00e-05
  [680/6080806] (0.0%) - Loss: 4.7952, LR: 5.00e-05
  [690/6080806] (0.0%) - Loss: 3.8334, LR: 5.00e-05
  [700/6080806] (0.0%) - Loss: 3.3808, LR: 5.00e-05
  [710/6080806] (0.0%) - Loss: 1.5567, LR: 5.00e-05
